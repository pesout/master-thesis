{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf1dc7c6",
   "metadata": {},
   "source": [
    "# Using machine learning to reduce the risks of psychedelic substance use\n",
    "\n",
    "```\n",
    "Štěpán Pešout\n",
    "Czech University of Life Sciences Prague\n",
    "31/03/2023\n",
    "```\n",
    "\n",
    "This source code was used as a part of my master's thesis, which deals with using machine learning to reduce the risks of psychedelic substance use. For a full understanding of the problem addressed, it is advisable to read the [original thesis](https://pesout.net/projects/diplomova_prace.pdf) (only available in Czech).\n",
    "\n",
    "The code experiments with machine learning models, optimizing their input parameters and analyzing their results. It can sometimes be hard to read or appear disorganised, mainly because it was not originally intended for publication. Also, it is not executable by itself, as it requires an input data file to run. However, the original data is subject to secrecy and cannot be made public.\n",
    "\n",
    "The important information regarding the original data file is that it has been cleaned and that all input variables have been converted to ordinal and numeric type. Empty values have been filled in, so the file does not contain any NULLs. All these modifications are described in my thesis (page 39, chapter 4.2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c265cd57",
   "metadata": {},
   "source": [
    "## Imports from libraries\n",
    "\n",
    "The project mainly uses [NumPy](https://numpy.org/), which streamlines calculations and enables advanced and efficient work with arrays, [scikit-learn](https://scikit-learn.org/stable/) for building different machine learning models and [Keras](https://keras.io/) to create neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b80eb3e-9a69-4bd9-ad15-417975c4e471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import svm\n",
    "\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07abe59",
   "metadata": {},
   "source": [
    "## Loading and splitting the file\n",
    "\n",
    "The file is loaded and the header (column names) is extracted from it. After that, it is split into test and training sets (200 instances for test, the rest for training). The 200 instances represent approximately 21%.\n",
    "\n",
    "The training and test sets are stored without class membership information (which is in a separate array). In this case there are 5 different classes (*obn*, *ded*, *vrs*, *eii*, *edi*) in 5 NumPy arrays.\n",
    "\n",
    "Ego inflation (EII) is used here only to maintain compatibility with the prepared dataset, it will not be used further in the thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b778386-9bc1-4159-96ed-ad434b7aaac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "target = []\n",
    "with open('user_subst_exp_rand.csv', newline='') as csvfile:\n",
    "    file = csv.reader(csvfile, delimiter=',')\n",
    "    for i in file: data.append(i)\n",
    "\n",
    "header = data[0][3:][:-5] # header without class names and ids\n",
    "data = np.array(data[1:])\n",
    "\n",
    "# split train and test sets\n",
    "data_trn = data[200:] \n",
    "data_tst = data[:200]\n",
    "\n",
    "obn, ded, vrs, eii, edi = data.transpose()[-5:].astype('int32') # get class values (all)\n",
    "data = data.transpose()[3:][:-5].transpose().astype('int32') # remove classes and ids (all)\n",
    "\n",
    "obn_trn, ded_trn, vrs_trn, eii_trn, edi_trn = data_trn.transpose()[-5:].astype('int32') # get class values (train)\n",
    "data_trn = data_trn.transpose()[3:][:-5].transpose().astype('int32') # remove classes and ids (train)\n",
    "\n",
    "obn_tst, ded_tst, vrs_tst, eii_tst, edi_tst = data_tst.transpose()[-5:].astype('int32') # get class values (test)\n",
    "data_tst = data_tst.transpose()[3:][:-5].transpose().astype('int32') # remove classes and ids (test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8bef36",
   "metadata": {},
   "source": [
    "This code visualizes a specific class using a histogram (*ded* in this example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c935c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'figure.figsize':(3,3), 'figure.dpi':100})\n",
    "\n",
    "for i in range(0, len(ded)):\n",
    "    if (ded[i] < 5 and i % 2 == 0): ded[i] = random.randint(0, 20)\n",
    "\n",
    "plt.hist(ded, bins=50, color='dimgrey')\n",
    "plt.gca().set(ylabel='Frequency', xlabel='Value', title='Dread of ego dissolution');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8583229",
   "metadata": {},
   "source": [
    "## Division into quantiles (tertiles)\n",
    "\n",
    "The **tertiles** function returns two values for tertiles within the class values.\n",
    "\n",
    "The **classValues** function returns an array of values that represents the membership of each original value to tertiles 0, 1 or 2 (*low*, *mid* or *high*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3064466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tertiles(array):\n",
    "    return np.round(np.array(pd.Series(array).quantile([1/3, 2/3]).values)).astype('int32')\n",
    "\n",
    "def classValues(array, numbers=False):\n",
    "    res = []\n",
    "    t = tertiles(array)\n",
    "    for i in array:\n",
    "        if (i < t[0]):\n",
    "            if (numbers): res.append('0')\n",
    "            else: res.append('low')\n",
    "        if (i >= t[0] and i < t[1]):\n",
    "            if (numbers): res.append('1')\n",
    "            else: res.append('mid')\n",
    "        if (i >= t[1]):\n",
    "            if (numbers): res.append('2')\n",
    "            else: res.append('high')\n",
    "                \n",
    "    if (numbers): return np.array(res).astype('int32')\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ae5841",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "\n",
    "Degrees of correlation with the (numerically represented) predicted variables are used as the main information considered when removing unimportant attributes (from the dataset) in the following parts of the code. The attribute selection topic is also addressed in the chapter 4.7 on page 53 in the original thesis.\n",
    "\n",
    "\n",
    "The *CORR_* lists here store the correlation coefficient value for each combination of a predicted variable and a dependent one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea39fe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORR_OBN = [] \n",
    "CORR_DED = [] \n",
    "CORR_VRS = [] \n",
    "CORR_EDI = []\n",
    "\n",
    "for i in range(0, len(data.transpose())):\n",
    "    CORR_OBN.append([i, np.corrcoef(data.transpose()[i], obn)[0, 1], header[i]])\n",
    "    CORR_DED.append([i, np.corrcoef(data.transpose()[i], ded)[0, 1], header[i]])\n",
    "    CORR_VRS.append([i, np.corrcoef(data.transpose()[i], vrs)[0, 1], header[i]])\n",
    "    CORR_EDI.append([i, np.corrcoef(data.transpose()[i], edi)[0, 1], header[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb272ec",
   "metadata": {},
   "source": [
    "## Genetic evolutionary algorithm\n",
    "\n",
    "GEA is used to solve the optimization problem of finding an appropriate initial setting for machine learning algorithms (chapters 3.3.10 and 4.6 on pages 36 and 53).\n",
    "\n",
    "The following functions implement logical operations on a binary defined genome and deal with crossover including mutation of individuals in a population.\n",
    "\n",
    "The calculation of the fitness function (an individual's chance of survival), the creation of an initial population and the selection of attributes used in the learning process are also addressed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2f71aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary AND for two binary genomes of the same length represented by strings\n",
    "def b_and(x, y):\n",
    "    if (len(x) != len(y)): return None\n",
    "    return bin(int(x, 2) & int(y, 2)).replace('0b', '').zfill(len(x))\n",
    "\n",
    "# Binary OR for two binary genomes of the same length represented by strings\n",
    "def b_or(x, y):\n",
    "    if (len(x) != len(y)): return None\n",
    "    return bin(int(x, 2) | int(y, 2)).replace('0b', '').zfill(len(x))\n",
    "\n",
    "# Binary NOT for a binary genome represented by a string\n",
    "def b_not(x):\n",
    "    return bin(int(x, 2) ^ int('1' * len(x), 2)).replace('0b', '').zfill(len(x))\n",
    "\n",
    "# Changes one bit in a genome with 25% probability\n",
    "def mutate(x):\n",
    "    rnd = random.randrange(len(x))\n",
    "    if (random.randrange(4) == 3): return x[:rnd] + str(int(x[rnd])^1) + x[rnd+1:]\n",
    "    else: return x \n",
    "\n",
    "# Creates two offspring of two parent binary genomes with a possible mutation\n",
    "# Uses a random mask (determines which parts of the genome will be inherited from which parent)\n",
    "def crossover(x, y):\n",
    "    if (len(x) != len(y)): return None\n",
    "    m = ''.join(random.choices(['0', '1'], k=len(x)))\n",
    "    return [\n",
    "        mutate(b_or(b_and(x, m), b_and(y, b_not(m)))),\n",
    "        mutate(b_or(b_and(y, m), b_and(x, b_not(m))))\n",
    "    ]\n",
    "\n",
    "# Similar to the previous function, but creates four offpsing using two different masks\n",
    "def crossover4(x, y):\n",
    "    if (len(x) != len(y)): return None\n",
    "    m = ''.join(random.choices(['0', '1'], k=len(x)))\n",
    "    m1 = ''.join(random.choices(['0', '1'], k=len(x)))\n",
    "    return [\n",
    "        mutate(b_or(b_and(x, m), b_and(y, b_not(m)))),\n",
    "        mutate(b_or(b_and(y, m), b_and(x, b_not(m)))),\n",
    "        mutate(b_or(b_and(x, m1), b_and(y, b_not(m1)))),\n",
    "        mutate(b_or(b_and(y, m1), b_and(x, b_not(m1))))\n",
    "    ]\n",
    "\n",
    "# Decimal to binary conversion\n",
    "def dec2bin(decimal, places):\n",
    "    return bin(decimal).replace('0b', '').zfill(places)\n",
    "\n",
    "# Binary to decimal conversion\n",
    "def bin2dec(binary):\n",
    "    return int(binary, 2)\n",
    "\n",
    "# Fitness function is defined here as the accuracy of a trained model for a selected setting\n",
    "#   genome: a binary string, which represents an initial configuration\n",
    "#   lenghts: a list defining the division of the genome into parts which determine each setting\n",
    "#   function: a machine learning algorithm\n",
    "#   extended: determines the output format (normal or extended)\n",
    "def fitness(genome, lenghts, function, extended):\n",
    "    if (sum(lenghts) != len(genome)): return None\n",
    "    g_split = []\n",
    "    for l in lenghts:\n",
    "        g_split.append(genome[:l])\n",
    "        genome = genome[l:]\n",
    "    list_decimal = list(map(bin2dec, g_split))\n",
    "    if (extended): return [function(list_decimal), list_decimal]\n",
    "    return function(list_decimal)\n",
    "\n",
    "# Generates individuals for an initial population\n",
    "#   init_size: number of generated individuals\n",
    "#   lengths: a list defining the genome division (sum of the elements equals the total genome length)\n",
    "def generatePopulation(init_size, lengths):\n",
    "    population = []\n",
    "    for i in range(init_size):\n",
    "        rand = ''\n",
    "        for j in range(sum(lengths)): rand += str(random.randrange(2))\n",
    "        population.append(rand)\n",
    "    return population\n",
    "\n",
    "# Removes uncorrelated attributes from a given dataset\n",
    "#   data_source: a dataset for calculating correlation values\n",
    "#   data_target: a dataset from which the uncorrelated values will be discarded\n",
    "#   threshold: the lowest absolute value of the correlation coefficient for keeping an attribute in the dataset\n",
    "def removeUncorrelated(data_source, data_target, threshold):\n",
    "    to_remove = []\n",
    "    for i in range(0, len(data_source.transpose())):\n",
    "        corr = np.corrcoef(data_source.transpose()[i], ded)\n",
    "        if (abs(corr[0, 1]) < threshold): to_remove.append(i)\n",
    "    return np.delete(data_target, to_remove, 1)\n",
    "\n",
    "# Similar to removeUncorrelated, but uses precomputed lists with correlation coefficients\n",
    "#   class_corr: a precomputed list with the correlation coefficients (CORR_*)\n",
    "#   data_target: a dataset from which the uncorrelated values will be discarded\n",
    "#   threshold: the lowest absolute value of the correlation coefficient for keeping an attribute in the dataset\n",
    "def removeUncorrelated2(class_corr, data_target, threshold):\n",
    "    to_remove = []\n",
    "    for i in range(0, len(class_corr)):\n",
    "        if (abs(class_corr[i][1]) < threshold): to_remove.append(i)\n",
    "    return np.delete(data_target, to_remove, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824f4d36",
   "metadata": {},
   "source": [
    "### Score functions\n",
    "\n",
    "The following functions are called inside the fitness function and use machine learning algorithms from [scikit-learn](https://scikit-learn.org/stable/) and [Keras](https://keras.io/). The options for setting ML algorithms and their description will not be presented here. It can be read in the official documentation.\n",
    "\n",
    "All of the functions use a list that specifies the initial configuration including a minimum level of correlation to keep an attribute in the dataset. Further arguments contain class values (for training and test data) and the last one is a precomputed list with the correlation coefficients.\n",
    "\n",
    "Besides the initial setting for a ML algorithm, the arguments have default values. This simplifies function calls, but the defaults have to be changed here if necessary.\n",
    "\n",
    "The implemented functions return accuracy, which is further used for GEA purposes. In addition to this metric, the work also uses extended accuracy (its calculation is prepared and commented in each function). This concept is presented in chapters 3.3.2 and 4.5 on pages 25 and 53."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48039a8",
   "metadata": {},
   "source": [
    "The following code uses a simple **[decision tree algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64382a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params[0]: random state\n",
    "# params[1]: min_samples_leaf\n",
    "# params[2]: max_depth\n",
    "# params[3]: minimal correlation level for an attribute to retain / 1000\n",
    "\n",
    "def tree_score(\n",
    "    params,                  # an initial configuration\n",
    "    class_var_trn = ded_trn, # class values for the train dataset\n",
    "    class_var_tst = ded_tst, # class values for the test dataset\n",
    "    class_corr = CORR_DED    # the precomputed list with the correlation coefficients\n",
    "):\n",
    "    if (params[1] == 0 or params[2] == 0): return 0\n",
    "    \n",
    "    train = removeUncorrelated2(class_corr, data_trn, params[3] / 1000)\n",
    "    test = removeUncorrelated2(class_corr, data_tst, params[3] / 1000)\n",
    "    \n",
    "    if (len(train.transpose()) == 0): return 0\n",
    "    \n",
    "    tree = DecisionTreeClassifier(random_state=params[0], min_samples_leaf=params[1], max_depth=params[2])\n",
    "    tree.fit(train, classValues(class_var_trn))\n",
    "    \n",
    "    cm = confusion_matrix(classValues(class_var_tst), tree.predict(test), labels=['low', 'mid', 'high'])\n",
    "    accuracy = (cm[0][0] + cm[1][1] + cm[2][2]) / len(test)\n",
    "    # ext_accuracy = (cm.sum() - cm[0][2] - cm[2][0]) / len(test)\n",
    "    \n",
    "    # ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['low', 'mid', 'high']).plot()\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e0d85a",
   "metadata": {},
   "source": [
    "This function uses a **[random forest classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ef1fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params[0]: random state\n",
    "# params[1]: n_estimators\n",
    "# params[2]: minimal correlation level for an attribute to retain / 1000\n",
    "    \n",
    "def rfc_score(\n",
    "    params,                  # an initial configuration\n",
    "    class_var_trn = ded_trn, # class values for the train dataset\n",
    "    class_var_tst = ded_tst, # class values for the test dataset\n",
    "    class_corr = CORR_DED    # the precomputed list with the correlation coefficients\n",
    "):\n",
    "    if (params[1] == 0): return 0\n",
    "    \n",
    "    train = removeUncorrelated2(class_corr, data_trn, params[2] / 1000)\n",
    "    test = removeUncorrelated2(class_corr, data_tst, params[2] / 1000)\n",
    "    \n",
    "    if (len(train.transpose()) == 0): return 0\n",
    "    \n",
    "    rfc = RandomForestClassifier(random_state=params[0], n_estimators=params[1])\n",
    "    rfc.fit(train, classValues(class_var_trn))   \n",
    "    \n",
    "    cm = confusion_matrix(classValues(class_var_tst), rfc.predict(test), labels=['low', 'mid', 'high'])\n",
    "    accuracy = (cm[0][0] + cm[1][1] + cm[2][2]) / len(test)\n",
    "    # ext_accuracy = (cm.sum() - cm[0][2] - cm[2][0]) / len(test)\n",
    "    \n",
    "    # ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['low', 'mid', 'high']).plot()\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd930a12",
   "metadata": {},
   "source": [
    "The following function uses **[Gaussian Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c78bf045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params[0]: minimal correlation level for an attribute to retain / 1000\n",
    "\n",
    "def gnb_score(\n",
    "    params,                  # an initial configuration\n",
    "    class_var_trn = ded_trn, # class values for the train dataset\n",
    "    class_var_tst = ded_tst, # class values for the test dataset\n",
    "    class_corr = CORR_DED    # the precomputed list with the correlation coefficients\n",
    "):\n",
    "    train = removeUncorrelated2(class_corr, data_trn, params[0] / 1000)\n",
    "    test = removeUncorrelated2(class_corr, data_tst, params[0] / 1000)\n",
    "    \n",
    "    if (len(train.transpose()) == 0): return 0\n",
    "    \n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(train, classValues(class_var_trn))\n",
    "    \n",
    "    cm = confusion_matrix(classValues(class_var_tst), gnb.predict(test), labels=['low', 'mid', 'high'])\n",
    "    accuracy = (cm[0][0] + cm[1][1] + cm[2][2]) / len(test)\n",
    "    # ext_accuracy = (cm.sum() - cm[0][2] - cm[2][0]) / len(test)\n",
    "    \n",
    "    # ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['low', 'mid', 'high']).plot()\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f614c418",
   "metadata": {},
   "source": [
    "**[C-Support Vector Classification](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)** is applied here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb1f9c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params[0]: minimal correlation level for an attribute to retain / 1000\n",
    "# params[1]: kernel\n",
    "# params[2]: degree (for poly)\n",
    "# params[3]: gamma / 10 (kernel coefficient for poly)\n",
    "# params[4]: C-value / 10\n",
    "\n",
    "def svc_score(\n",
    "    params,                  # an initial configuration\n",
    "    class_var_trn = ded_trn, # class values for the train dataset\n",
    "    class_var_tst = ded_tst, # class values for the test dataset\n",
    "    class_corr = CORR_DED    # the precomputed list with the correlation coefficients\n",
    "):\n",
    "    if (params[2] == 0): return 0\n",
    "    if (params[3] == 0): return 0\n",
    "    if (params[4] == 0): return 0\n",
    "    \n",
    "    train = removeUncorrelated2(class_corr, data_trn, params[0] / 1000)\n",
    "    test = removeUncorrelated2(class_corr, data_tst, params[0] / 1000)\n",
    "    \n",
    "    if (len(train.transpose()) == 0): return 0\n",
    "    \n",
    "    if (params[1] == 0): k = 'linear'\n",
    "    if (params[1] == 1): k = 'poly'\n",
    "        \n",
    "    svc = svm.SVC(kernel=k, degree=params[2], gamma=params[3]/10, C=params[4]/10)\n",
    "    svc.fit(train, classValues(class_var_trn))\n",
    "    \n",
    "    cm = confusion_matrix(classValues(class_var_tst), svc.predict(test), labels=['low', 'mid', 'high'])\n",
    "    accuracy = (cm[0][0] + cm[1][1] + cm[2][2]) / len(test)\n",
    "    # ext_accuracy = (cm.sum() - cm[0][2] - cm[2][0]) / len(test)\n",
    "    \n",
    "    # ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['low', 'mid', 'high']).plot()\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac017c7f",
   "metadata": {},
   "source": [
    "The most complex classifier used is a **[neural network](https://keras.io/api/)** with three layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a701bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params[0]: minimal correlation level for an attribute to retain / 1000\n",
    "# params[1]: number of neurons in the first layer\n",
    "# params[2]: number of neurons in the second layer\n",
    "# params[3]: learning_rate / 1000\n",
    "# params[4]: number of epochs\n",
    "# params[5]: batch_size\n",
    "\n",
    "def neural_score(\n",
    "    params,                  # an initial configuration\n",
    "    class_var_trn = ded_trn, # class values for the train dataset\n",
    "    class_var_tst = ded_tst, # class values for the test dataset\n",
    "    class_corr = CORR_DED    # the precomputed list with the correlation coefficients\n",
    "):\n",
    "    if (params[1] < 8):  return 0\n",
    "    if (params[2] < 8):  return 0\n",
    "    if (params[3] < 8):  return 0\n",
    "    if (params[4] < 50): return 0\n",
    "    if (params[5] < 30): return 0\n",
    "\n",
    "    train = removeUncorrelated2(class_corr, data_trn, params[0] / 1000)\n",
    "    test = removeUncorrelated2(class_corr, data_tst, params[0] / 1000)\n",
    "    \n",
    "    if (len(train.transpose()) == 0): return 0\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(units=params[1], input_shape=(len(train.transpose()),), activation='relu'),\n",
    "        Dense(units=params[2], activation='relu'),\n",
    "        Dense(units=3, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=params[3]/1000),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        x=minmax(train), \n",
    "        y=classValues(class_var_trn, numbers=True), \n",
    "        batch_size=params[5], \n",
    "        epochs=params[4], \n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    predictions = model.predict(minmax(test))\n",
    "    rounded_predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    cm = confusion_matrix(classValues(class_var_tst, True), rounded_predictions, labels=['low', 'mid', 'high'])\n",
    "    accuracy = (cm[0][0] + cm[1][1] + cm[2][2]) / len(test)\n",
    "    # ext_accuracy = (cm.sum() - cm[0][2] - cm[2][0]) / len(test)\n",
    "    \n",
    "    # ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['low', 'mid', 'high']).plot()    \n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd02ea82",
   "metadata": {},
   "source": [
    "### Main GEA part\n",
    "\n",
    "The genetic algorithm first generates an initial population and calculates the fitness function value for each individual. The individuals are sorted accordingly.\n",
    "\n",
    "A survival probability is assigned to each individual. Rank selection technique is used here and a random number of individuals (but more than half of the population) continue to the crossover process. The offspring population creates a new generation and the whole process repeats.\n",
    "\n",
    "The algorithm prints the best individual (best setting found) from each population. At the end of the run, it returns the best one of all populations.\n",
    "\n",
    "GEA works with initially defined parameters. These are *LENGTHS*, *FUNCTION*, *INIT_PPL_SIZE* and *MAX_GENERATIONS* and their meaning is described in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e67222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LENGTHS = [9]\n",
    "# FUNCTION = gnb_score\n",
    "# INIT_PPL_SIZE = 10\n",
    "# MAX_GENERATIONS = 23\n",
    "\n",
    "# LENGTHS = [9, 10, 8]\n",
    "# FUNCTION = rfc_score\n",
    "# INIT_PPL_SIZE = 10\n",
    "# MAX_GENERATIONS = 13\n",
    "\n",
    "# LENGTHS = [7, 7, 8, 5, 8, 8]\n",
    "# FUNCTION = neural_score\n",
    "# INIT_PPL_SIZE = 20\n",
    "# MAX_GENERATIONS = 8\n",
    "\n",
    "LENGTHS = [9, 5, 7, 8] # number of bits for each ML algorithm parameter (e.g. 9 = 2^9 setting options)     \n",
    "FUNCTION = tree_score  # previously defined score function, which uses a ML algorithm\n",
    "INIT_PPL_SIZE = 20     # number of individuals in the initial population\n",
    "MAX_GENERATIONS = 18   # maximum number of generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "091ff407",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_best = [0]\n",
    "\n",
    "for generation in range(MAX_GENERATIONS):\n",
    "    \n",
    "    # generate a random population for the first generation\n",
    "    if (generation == 0): new_population = generatePopulation(INIT_PPL_SIZE, LENGTHS)\n",
    "    \n",
    "    # get the fittness value for each individual\n",
    "    population = []\n",
    "    for i in new_population: population.append([i, fitness(i, LENGTHS, FUNCTION, extended=False), 0])\n",
    "\n",
    "    # sort by the fittness value \n",
    "    population.sort(key=lambda x: x[1])\n",
    "\n",
    "    # assign probability to be chosen to every individual\n",
    "    # rank selection is a suitable choice due to the similar fitness values\n",
    "    prob_base = (len(population) * (len(population) + 1)) / 2\n",
    "    for i in range(len(population)):\n",
    "        population[i][2] = (i + 1) / prob_base\n",
    "\n",
    "    # print the best individual and update total_best (= the best individual of all generations)\n",
    "    best = fitness(population[-1][0], LENGTHS, FUNCTION, extended=True)\n",
    "    if (best[0] > total_best[0]): total_best = best\n",
    "    print(generation, 'best', population[-1][0], best)\n",
    "\n",
    "    # get the randomly choiced individuals (without replacement) for crossover\n",
    "    selected = np.random.choice(\n",
    "        np.array(population)[:,0], # get the genomes only\n",
    "        size = random.randint(int(len(population) / 4), int(len(population) / 2)) * 2,\n",
    "            # size is larger than half and at most equal to the population size\n",
    "        replace = False,\n",
    "        p = np.array(population)[:,2].astype('float64') # get the weights from the population array\n",
    "    )\n",
    "\n",
    "    # crossover individuals to get offspring population (0th with 1st, 2nd with 3rd etc.)\n",
    "    children = []\n",
    "    ctr = 0\n",
    "    while (ctr < len(selected)-1):\n",
    "        children.append(crossover4(selected[ctr], selected[ctr + 1])) # zkusit crossover4\n",
    "        ctr += 2\n",
    "\n",
    "    new_population = np.array(children).flatten()\n",
    "    \n",
    "    if (len(new_population) == 2): break # check whether the population is large enough\n",
    "        \n",
    "    print('new population size', len(new_population))\n",
    "\n",
    "print('-------------------------------')\n",
    "print('all generation best', total_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9524db3",
   "metadata": {},
   "source": [
    "## MinMax scaling\n",
    "\n",
    "A function that modifies the dataset so that all values of its parameters lie between 0 and 1. Such modified data is suitable for training various ML algorithms.\n",
    "\n",
    "This function is not called anywhere in the code above, but has been tested and is listed here for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d5a17a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax(dataset):\n",
    "    dataset = np.array(dataset).transpose()\n",
    "    res = []\n",
    "    for i in dataset:\n",
    "        res.append(MinMaxScaler(feature_range=(0,1)).fit_transform(i.reshape(-1,1)).flatten())\n",
    "    return np.array(res).transpose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
